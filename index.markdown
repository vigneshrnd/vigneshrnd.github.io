---
layout: page
title: About
---

<!---
<iframe src="https://drive.google.com/file/d/1ndWdniSFbnLg08xCQNwwZ40-XH2eNOwh/preview" width="640" height="480"></iframe>

-->

<img src="assets/VigneshImage.jpeg" style="float: left;" alt="drawing"/>

---

<br/>
Welcome! I am Vignesh, and I work at Apple on AI/ML. I spealize in leading Artificial Intelligence initiatives blending foundational core technology development and user experiences. I have been responsible for several important inflection points in infusing multimodal ML workstreams into the Apple Ecosystem

**Today,** I lead three fantastic teams that focus on visual generation, retreival / reasoning & AI Safety. Our north star is to influence user experiences through state of the art AI. 

**Apple Intelligence,** has been a primary area of focus for my teams over the past few years. Currently lead a diverse set of focus areas including model pre-training, post-training, opitmization, alignment, indexing & reasoning. We interface with a variety of partner teams cutting across disciplines in service of user experiences.

**Joining Apple** as a Research Engineer in 2014, I had the opportunity to be part of the core team that laid the foundations for on-device ML in the company. Initial efforts on enabling face recognition and scene analysis evolved alongside the infrastructure for efficient on-device inference. Over the next several years, worked on architecting core ML workstreams for Camera and Photos products. 

**From 2019 onwards,** I have been focussing a lot more on systemwide intelligence. Live Text, Visual Lookup, Live Stickers, Communication Safety, Child Safety and Accesibility are among the experiences we focussed on. I am especially proud of the work on image captioning that we shipped in 2020. It was the best in class on-device captioner, and laid the foundations for infrastructure that supported efficient inference of transformers. This gradually led to dedicated efforts on efficient transformer inference, their use in photographic styles, and eventually to the work on on-device diffusion models.

**At a very high level,** overview of products and core technologies I have led:

| Product Surface Area      | Projects |
| ----------- | ----------- |
| **Systemwide Intelligence**   | Systemwide Visual Generation, Visual Intelligence (Lookup, Search), Code Scanning.       |
| **Developer Experience**   | 3rd party APIs in Vision.fwk, 3rd party Safety APIs, CreateML / CoreML for custom training.        |
| **Communication Apps**   | Live Stickers, Communication Safety, Genmoji, Generative Messages Background.         |
| **Apple Camera Experiences**   | Photographic Styles & Semantic Segmentation, Portrait Mode, QR Code Scanning, Live Text.         |
|  **Apple Photos**   | Apple Intelligence Memories, Photos Curation, Photos Search, Personalization.         |


| Core Technologies      | Projects |
| ----------- | ----------- |
| **Visual Generation**   | Spearheaded on-device image generation / diffusion, Apple Diffusion Model (ADM) pre-training / post-training and optimization.       |
| **Infrastructure**   | Training workflows for distributed training of diffusion  embedding models, Apple Neural Engine optimized deployment of transformer models.       |
| **Safety**   | Training and deployment of systemwide guardrails that power Apple  Intelligence. Alignment / model mitigation post training.       |
| **Multimodal Embeddings**   |  The primary embedding model that is used across Apple operating systems. Also resulted in image/video captioning that allows for efficient descriptions of visual content in performance constrained settings.        |
| **Child Safety**   | Led the development of communication safety models for images and video.       |
| **Systemwide Visual Perception**    | Fundamental image tagging, image saliency estimation, aesthetics, embeddings, custom classifiers are extensively used for visual analysis across the system.    |